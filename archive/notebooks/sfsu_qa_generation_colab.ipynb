{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SFSU Q&A Generation with Ollama (FREE on Google Colab)\n",
        "\n",
        "This notebook will generate Q&A training pairs from your SFSU scraped data.\n",
        "\n",
        "**Features:**\n",
        "- ‚úÖ FREE (uses Google's free GPU)\n",
        "- ‚úÖ No PC load/overheating\n",
        "- ‚úÖ Process ALL 3,867 pages\n",
        "- ‚úÖ ~60 minutes total time\n",
        "- ‚úÖ Generate ~10,000 Q&A pairs\n",
        "\n",
        "**Steps:**\n",
        "1. Run Cell 1: Install Ollama\n",
        "2. Run Cell 2: Upload your data file\n",
        "3. Run Cell 3: Generate Q&A pairs\n",
        "4. Run Cell 4: Download results"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Install and Start Ollama"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-ollama"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "print(\"üì¶ Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama server in background\n",
        "print(\"\\nüöÄ Starting Ollama server...\")\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "time.sleep(10)  # Wait for server to start\n",
        "\n",
        "# Pull the model\n",
        "print(\"‚¨áÔ∏è Downloading llama3.2 model (this may take a few minutes)...\")\n",
        "!ollama pull llama3.2\n",
        "\n",
        "print(\"\\n‚úÖ Ollama is ready!\")\n",
        "print(\"GPU Available:\", \"YES\" if subprocess.run(['nvidia-smi'], capture_output=True).returncode == 0 else \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Upload Your Data File"
      ],
      "metadata": {
        "id": "upload"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"üì§ Please upload your comprehensive_sfsu_crawl.json file\")\n",
        "print(\"   (Click 'Choose Files' and select the file from your D:\\\\sfsu-cs-chatbot\\\\data folder)\")\n",
        "print()\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Move uploaded file\n",
        "for filename in uploaded.keys():\n",
        "    !mv \"{filename}\" data/comprehensive_sfsu_crawl.json\n",
        "    print(f\"\\n‚úÖ Uploaded: {filename}\")\n",
        "    \n",
        "    import json\n",
        "    with open('data/comprehensive_sfsu_crawl.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"   Pages in file: {len(data)}\")"
      ],
      "metadata": {
        "id": "upload-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Generate Q&A Pairs"
      ],
      "metadata": {
        "id": "generate"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generate_qa.py\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def call_ollama(prompt):\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": \"llama3.2\",\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False,\n",
        "        \"options\": {\"temperature\": 0.7, \"num_predict\": 1500}\n",
        "    }\n",
        "    response = requests.post(url, json=payload, timeout=60)\n",
        "    return response.json()['response'].strip()\n",
        "\n",
        "def extract_qa(page):\n",
        "    url = page.get('url', '')\n",
        "    title = page.get('title', 'No Title')\n",
        "    text = page.get('full_text', '')\n",
        "    \n",
        "    if not text or len(text) < 200:\n",
        "        return []\n",
        "    \n",
        "    if len(text) > 3000:\n",
        "        text = text[:3000]\n",
        "    \n",
        "    prompt = f'''Extract 2-3 natural Q&A pairs from this SFSU content.\n",
        "\n",
        "Title: {title}\n",
        "Content: {text}\n",
        "\n",
        "Return ONLY valid JSON array:\n",
        "[{{\"question\": \"...\", \"answer\": \"...\", \"source_url\": \"{url}\"}}]\n",
        "\n",
        "JSON:'''\n",
        "    \n",
        "    try:\n",
        "        result = call_ollama(prompt)\n",
        "        if '```json' in result:\n",
        "            result = result.split('```json')[1].split('```')[0]\n",
        "        elif '```' in result:\n",
        "            result = result.split('```')[1].split('```')[0]\n",
        "        return json.loads(result.strip())\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "with open('data/comprehensive_sfsu_crawl.json', 'r') as f:\n",
        "    pages = json.load(f)\n",
        "\n",
        "valid = [p for p in pages if p.get('status')=='success' and p.get('full_text') and len(p.get('full_text',''))>200]\n",
        "print(f\"Processing {len(valid)} valid pages...\\n\")\n",
        "\n",
        "all_qa = []\n",
        "start = time.time()\n",
        "\n",
        "for i, page in enumerate(valid, 1):\n",
        "    title = page.get('title', '')[:50]\n",
        "    print(f\"[{i}/{len(valid)}] {title}...\", end=' ')\n",
        "    \n",
        "    qa_pairs = extract_qa(page)\n",
        "    if qa_pairs:\n",
        "        all_qa.extend(qa_pairs)\n",
        "        print(f\"‚úì +{len(qa_pairs)} (Total: {len(all_qa)})\")\n",
        "    else:\n",
        "        print(\"‚úó\")\n",
        "    \n",
        "    if i % 50 == 0:\n",
        "        elapsed = time.time() - start\n",
        "        rate = i / elapsed\n",
        "        remaining = (len(valid) - i) / rate / 60\n",
        "        print(f\"\\n[CHECKPOINT] {len(all_qa)} pairs | ETA: {remaining:.0f} min\\n\")\n",
        "        with open('data/qa_training_data.json', 'w') as f:\n",
        "            json.dump(all_qa, f, indent=2)\n",
        "\n",
        "# Save final\n",
        "with open('data/qa_training_data.json', 'w') as f:\n",
        "    json.dump(all_qa, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Complete! Generated {len(all_qa)} Q&A pairs in {(time.time()-start)/60:.1f} minutes\")"
      ],
      "metadata": {
        "id": "generate-qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the generation\n",
        "print(\"üöÄ Starting Q&A generation...\")\n",
        "print(\"This will take ~60 minutes for all pages.\\n\")\n",
        "\n",
        "!python generate_qa.py"
      ],
      "metadata": {
        "id": "run-generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Download Results"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Show statistics\n",
        "with open('data/qa_training_data.json', 'r') as f:\n",
        "    qa_data = json.load(f)\n",
        "\n",
        "print(\"üìä Final Statistics:\")\n",
        "print(f\"   Total Q&A pairs: {len(qa_data)}\")\n",
        "print(f\"   Unique pages: {len(set(qa['source_url'] for qa in qa_data))}\")\n",
        "print(f\"   Avg per page: {len(qa_data)/len(set(qa['source_url'] for qa in qa_data)):.2f}\")\n",
        "\n",
        "print(\"\\nüì• Downloading results...\")\n",
        "files.download('data/qa_training_data.json')\n",
        "\n",
        "print(\"\\n‚úÖ Done! Upload this file to your Supabase database.\")"
      ],
      "metadata": {
        "id": "download-results"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
