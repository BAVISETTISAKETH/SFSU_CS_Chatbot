from langchain_chroma import Chroma
from sentence_transformers import SentenceTransformer
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

DB_DIR = "./vector_db"

class SentenceTransformerEmbeddings:
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        
    def embed_documents(self, texts):
        embeddings = self.model.encode(texts, show_progress_bar=True)
        return embeddings.tolist()
        
    def embed_query(self, text):
        embedding = self.model.encode([text])[0]
        return embedding.tolist()

class HuggingFaceModel:
    def __init__(self, model_name="google/flan-t5-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # Move to GPU if available
        if torch.cuda.is_available():
            self.model = self.model.to("cuda")
            print("Model moved to GPU")
    
    def __call__(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        # Move inputs to same device as model
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        # Generate response
        output = self.model.generate(
            **inputs, 
            max_length=512,
            temperature=0.7
        )
        
        response = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return response

def test_retrieval(query, k=3):
    """Test just the retrieval component to see what documents are being retrieved."""
    print(f"\nTesting retrieval for query: '{query}'")
    print("-" * 50)
    
    embeddings = SentenceTransformerEmbeddings()
    vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    
    docs = vectorstore.similarity_search(query, k=k)
    
    print(f"Retrieved {len(docs)} documents:\n")
    for i, doc in enumerate(docs):
        print(f"Document {i+1} (Source: {doc.metadata.get('source', 'unknown')}):")
        print(f"{doc.page_content[:200]}...")
        print()
    
    return docs

def test_qa(query):
    """Test the entire QA chain with a query."""
    print(f"\nTesting QA for query: '{query}'")
    print("-" * 50)
    
    # Custom prompt template
    template = """You are a helpful assistant for the Computer Science Department at San Francisco State University (SFSU).
    Use the following pieces of context to answer the user's question.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
    Format your response in a clean, readable way. If listing courses or faculty, use a structured format.

    Context:
    {context}

    Question: {question}
    Answer:"""

    PROMPT = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    
    embeddings = SentenceTransformerEmbeddings()
    vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    
    # Load language model
    try:
        # Try to use a Hugging Face model (faster on Colab with GPU)
        llm = HuggingFaceModel("google/flan-t5-base")
        print("Using Hugging Face model with GPU acceleration")
    except Exception as e:
        print(f"Error loading Hugging Face model: {str(e)}")
        print("Falling back to simpler approach...")
        
        # Define a simple completion function
        def simple_completion(prompt):
            print("\nUsing simplified model. In production, you would use a full LLM.")
            print("Context and query received. In a real deployment, an LLM would generate a response here.")
            return "This is a placeholder response. In a real deployment, this would be generated by an LLM."
        
        llm = simple_completion
    
    # Create a simple QA function
    def qa_with_context(query):
        # Retrieve relevant documents
        docs = vectorstore.similarity_search(query, k=4)
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Format the prompt
        prompt = PROMPT.format(context=context, question=query)
        
        # Get response from LLM
        response = llm(prompt)
        
        return {"result": response}
    
    print("Processing query, this may take a moment...")
    start_time = time.time()
    
    try:
        response = qa_with_context(query)
        end_time = time.time()
        
        print(f"\nResponse (took {end_time - start_time:.2f} seconds):")
        print("-" * 30)
        print(response["result"])
        print("-" * 30)
    except Exception as e:
        print(f"Error getting response: {str(e)}")
    
    return {"result": "See output above"}

def run_tests():
    # Test queries relevant to your CS department data
    test_queries = [
        "What courses are required for the CS major?",
        "Who are the faculty members in the CS department?",
        "What are the prerequisites for CSC 413?",
        "How can I contact the department chair?",
        "What are the graduation requirements for CS students?",
    ]
    
    # Test each query with both retrieval and QA
    for query in test_queries:
        # First test retrieval only
        test_retrieval(query)
        
        # Then test the full QA chain
        test_qa(query)
        
        print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    run_tests()