"""
Test Script for Dual-Source RAG System
Tests all critical components and validates zero-hallucination architecture
"""

import asyncio
import sys
import os

# Add parent directory to path to import backend modules
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from services.dual_source_rag import DualSourceRAG
from services.context_merger import ContextMerger
from services.llm import LLMService
from dotenv import load_dotenv

# Load environment
load_dotenv()


class DualSourceTester:
    """Test the dual-source RAG system."""

    def __init__(self):
        """Initialize services for testing."""
        print("Initializing dual-source RAG system...")
        self.dual_source_rag = DualSourceRAG()
        self.context_merger = ContextMerger()
        self.llm_service = LLMService()

    async def test_scenario_1_both_sources(self):
        """Test Scenario 1: Both sources have information."""
        print("\n" + "="*70)
        print("TEST SCENARIO 1: Both Sources Have Information")
        print("="*70)

        query = "What are the Computer Science program requirements at SFSU?"

        # Step 1: Retrieve from both sources
        print(f"\n[TEST] Query: {query}")
        print("[TEST] Retrieving from both sources in parallel...")
        dual_results = await self.dual_source_rag.retrieve_all_sources(query)

        # Verify both sources were used
        assert dual_results['both_sources_used'], "‚ùå FAIL: Both sources not used"
        print(f"‚úÖ PASS: Both sources used")

        vector_count = dual_results['vector_count']
        web_count = dual_results['web_count']

        print(f"[TEST] Vector DB: {vector_count} documents")
        print(f"[TEST] Web Search: {web_count} results")

        # Step 2: Merge contexts
        print("\n[TEST] Merging contexts...")
        merged = self.context_merger.merge_contexts(
            vector_results=dual_results['vector_results'],
            web_results=dual_results['web_results'],
            query=query
        )

        print(f"‚úÖ PASS: Context merged ({merged['total_chars']} chars)")

        # Step 3: Generate response
        print("\n[TEST] Generating response with temperature 0.0...")
        llm_result = await self.llm_service.generate_dual_source_response(
            query=query,
            combined_context=merged['combined_context']
        )

        response = llm_result['response']
        print(f"\n[RESPONSE]\n{response}\n")

        # Verify citations
        has_local = '[Local]' in response or '[local]' in response
        has_web = '[Web]' in response or '[web]' in response

        if has_local and has_web:
            print(f"‚úÖ PASS: Response has citations from BOTH sources")
        elif has_local or has_web:
            print(f"‚ö†Ô∏è  WARNING: Response has citations from only ONE source")
        else:
            print(f"‚ùå FAIL: Response has NO citations")

        # Verify validation
        if llm_result.get('validated'):
            print(f"‚úÖ PASS: Response passed validation")
        else:
            print(f"‚ùå FAIL: Response failed validation")
            print(f"Warnings: {llm_result.get('validation_warnings', [])}")

        print(f"\n[METRICS]")
        print(f"  Citation count: {llm_result.get('citation_count', 0)}")
        print(f"  Validated: {llm_result.get('validated', False)}")
        print(f"  Has citations: {llm_result.get('has_citations', False)}")

        return llm_result.get('validated', False)

    async def test_scenario_2_vector_only(self):
        """Test Scenario 2: Only vector DB has information."""
        print("\n" + "="*70)
        print("TEST SCENARIO 2: Only Vector DB Has Information")
        print("="*70)

        # Use a query likely to have info in vector DB but not current web
        query = "Tell me about SFSU faculty members in Computer Science"

        print(f"\n[TEST] Query: {query}")
        print("[TEST] Retrieving from both sources...")
        dual_results = await self.dual_source_rag.retrieve_all_sources(query)

        vector_count = dual_results['vector_count']
        web_count = dual_results['web_count']

        print(f"[TEST] Vector DB: {vector_count} documents")
        print(f"[TEST] Web Search: {web_count} results")

        # Merge and generate
        merged = self.context_merger.merge_contexts(
            vector_results=dual_results['vector_results'],
            web_results=dual_results['web_results'],
            query=query
        )

        llm_result = await self.llm_service.generate_dual_source_response(
            query=query,
            combined_context=merged['combined_context']
        )

        response = llm_result['response']
        print(f"\n[RESPONSE]\n{response}\n")

        # Check for appropriate citation
        has_local = '[Local]' in response or '[local]' in response

        if has_local:
            print(f"‚úÖ PASS: Response cites [Local] source")
        else:
            print(f"‚ùå FAIL: Response missing [Local] citation")

        return llm_result.get('validated', False)

    async def test_scenario_3_neither_source(self):
        """Test Scenario 3: Neither source has information."""
        print("\n" + "="*70)
        print("TEST SCENARIO 3: Neither Source Has Information")
        print("="*70)

        # Intentionally obscure query
        query = "What is the molecular structure of the quantum computing lab refrigeration system?"

        print(f"\n[TEST] Query: {query}")
        print("[TEST] Retrieving from both sources...")
        dual_results = await self.dual_source_rag.retrieve_all_sources(query)

        vector_count = dual_results['vector_count']
        web_count = dual_results['web_count']

        print(f"[TEST] Vector DB: {vector_count} documents")
        print(f"[TEST] Web Search: {web_count} results")

        # Even with no results, system should handle gracefully
        merged = self.context_merger.merge_contexts(
            vector_results=dual_results['vector_results'],
            web_results=dual_results['web_results'],
            query=query
        )

        llm_result = await self.llm_service.generate_dual_source_response(
            query=query,
            combined_context=merged['combined_context']
        )

        response = llm_result['response']
        print(f"\n[RESPONSE]\n{response}\n")

        # Check for honest admission
        admission_phrases = [
            "don't have that information",
            "not available",
            "couldn't find",
            "no information"
        ]

        admits_missing = any(phrase in response.lower() for phrase in admission_phrases)

        if admits_missing:
            print(f"‚úÖ PASS: System honestly admits lack of information")
        else:
            print(f"‚ùå FAIL: System should admit when information is not available")

        return True  # Always pass if system doesn't crash

    async def test_temperature_zero(self):
        """Test that temperature is actually 0.0."""
        print("\n" + "="*70)
        print("TEST: Temperature 0.0 Verification")
        print("="*70)

        query = "What is the CS program?"

        # Run same query twice - should get identical responses at temp 0.0
        print("[TEST] Running same query twice to verify determinism...")

        results1 = await self.dual_source_rag.retrieve_all_sources(query)
        merged1 = self.context_merger.merge_contexts(
            vector_results=results1['vector_results'],
            web_results=results1['web_results'],
            query=query
        )
        llm1 = await self.llm_service.generate_dual_source_response(
            query=query,
            combined_context=merged1['combined_context']
        )

        # Second run
        results2 = await self.dual_source_rag.retrieve_all_sources(query)
        merged2 = self.context_merger.merge_contexts(
            vector_results=results2['vector_results'],
            web_results=results2['web_results'],
            query=query
        )
        llm2 = await self.llm_service.generate_dual_source_response(
            query=query,
            combined_context=merged2['combined_context']
        )

        response1 = llm1['response']
        response2 = llm2['response']

        # At temperature 0.0, responses should be very similar (nearly identical)
        # Allow for minor variations due to web search timing
        similarity = self._calculate_similarity(response1, response2)

        print(f"[TEST] Similarity between runs: {similarity:.1f}%")

        if similarity > 90:
            print(f"‚úÖ PASS: Responses are highly deterministic (temp 0.0 working)")
        elif similarity > 70:
            print(f"‚ö†Ô∏è  WARNING: Responses are similar but not identical ({similarity:.1f}%)")
        else:
            print(f"‚ùå FAIL: Responses are too different ({similarity:.1f}%) - check temperature setting")

        return similarity > 70

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate simple similarity between two texts."""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        return (len(intersection) / len(union)) * 100

    async def run_all_tests(self):
        """Run all test scenarios."""
        print("\n" + "="*70)
        print("DUAL-SOURCE RAG SYSTEM - COMPREHENSIVE TESTS")
        print("="*70)

        results = {}

        try:
            results['scenario_1'] = await self.test_scenario_1_both_sources()
        except Exception as e:
            print(f"‚ùå Scenario 1 failed: {e}")
            results['scenario_1'] = False

        try:
            results['scenario_2'] = await self.test_scenario_2_vector_only()
        except Exception as e:
            print(f"‚ùå Scenario 2 failed: {e}")
            results['scenario_2'] = False

        try:
            results['scenario_3'] = await self.test_scenario_3_neither_source()
        except Exception as e:
            print(f"‚ùå Scenario 3 failed: {e}")
            results['scenario_3'] = False

        try:
            results['temperature'] = await self.test_temperature_zero()
        except Exception as e:
            print(f"‚ùå Temperature test failed: {e}")
            results['temperature'] = False

        # Summary
        print("\n" + "="*70)
        print("TEST SUMMARY")
        print("="*70)

        total_tests = len(results)
        passed_tests = sum(1 for result in results.values() if result)

        for test_name, passed in results.items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"{status}: {test_name}")

        print(f"\nOverall: {passed_tests}/{total_tests} tests passed")

        if passed_tests == total_tests:
            print("\nüéâ SUCCESS: All tests passed! System is production-ready.")
            return True
        else:
            print("\n‚ö†Ô∏è  WARNING: Some tests failed. Review above for details.")
            return False


async def main():
    """Main test runner."""
    tester = DualSourceTester()
    success = await tester.run_all_tests()

    if success:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
